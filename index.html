<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>MuLAn Dataset</title>
<!--   <link rel="icon" type="image/x-icon" href="static/images/favicon.ico"> -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>

<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-2 publication-title">MULAN: A Multi Layer Annotated Dataset for Controllable Text-to-Image Generation</h1>
            <h1 class="title is-2 publication-title">CVPR 2024</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://uk.linkedin.com/in/petru-daniel-tudosiu" target="_blank">Petru-Daniel Tudosiu</a><sup>* 1</sup>,</span>
                <span class="author-block">
                  <a href="http://yang.ac/" target="_blank">Yongxin Yang</a><sup>1</sup>,</span>
                  <span class="author-block">
                    <a href="https://github.com/zsffq999" target="_blank">Shifeng Zhang</a><sup>1</sup>,</span>
                  <span class="author-block">
                  <a href="https://feierustc.github.io/" target="_blank">Fei Chen</a><sup>1</sup>,</span>
                  <span class="author-block">
                  <a href="https://smcdonagh.github.io/" target="_blank">Steven Mcdonagh</a><sup>2</sup>,</span>
                  <span class="author-block">
                    <a href="https://glampouras.github.io/" target="_blank">Gerasimos Lampouras</a><sup>1</sup>,</span>
                  <span class="author-block">
                    <a href="https://uk.linkedin.com/in/iiacobac" target="_blank">Ignacio Iacobacci</a><sup>1</sup>,</span>
                  <span class="author-block">
                    <a href="https://parisots.github.io" target="_blank">Sarah Parisot</a><sup>* 1</sup>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup> Huawei Noah's Ark Lab, <sup>2</sup> University of Edinburgh<br>
<!--                      Conference name and year-->
                      </span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- CVPR link -->
                     <span class="link-block">
                       <a href="https://openaccess.thecvf.com/content/CVPR2024/html/Tudosiu_MULAN_A_Multi_Layer_Annotated_Dataset_for_Controllable_Text-to-Image_Generation_CVPR_2024_paper.html" target="_blank"
                       class="external-link button is-normal is-rounded is-dark">
                       <span class="icon">
                         <i class="fas fa-file-pdf"></i>
                       </span>
                       <span>Paper</span>
                     </a>
                    </span>

                    <!-- Supplementary PDF link -->
<!--                    <span class="link-block">-->
<!--                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"-->
<!--                      class="external-link button is-normal is-rounded is-dark">-->
<!--                      <span class="icon">-->
<!--                        <i class="fas fa-file-pdf"></i>-->
<!--                      </span>-->
<!--                      <span>Supplementary</span>-->
<!--                    </a>-->
<!--                  </span>-->

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2404.02790" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://huggingface.co/datasets/mulan-dataset/v1.0" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Dataset</span>
                  </a>
                </span>

                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div>
        <img id="banner_sliding" src="static/images/banner_1.png">
      </div>
      <h2 class="subtitle has-text-centered">
        Our MuLAn dataset provides multi-layer image decomposition annotations, inpainting occluded areas and
          isolating instances in a scene on separate RGBA layers.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Text-to-image generation has achieved astonishing results, yet precise spatial controllability and prompt
              fidelity remain highly challenging. This limitation is typically addressed through cumbersome prompt
              engineering,  scene layout conditioning, or image editing techniques which often require hand drawn masks.
              Nonetheless, pre-existing works struggle to take advantage of the natural instance-level compositionality
              of scenes due to the typically flat nature of rasterized RGB output images. Towards adressing this
              challenge, we introduce MuLAn: a novel dataset comprising over 44K MUlti-Layer ANnotations of RGB images
              as multi-layer, instance-wise RGBA decompositions, and over 100K instance images. To build MuLAn, we
              developed a training free pipeline which decomposes a monocular RGB image into a stack of RGBA layers
              comprising of background and isolated instances. We achieve this through the use of pretrained
              general-purpose models, and by developing three modules: image decomposition for instance discovery
              and extraction, instance completion to reconstruct occluded areas, and image re-assembly.
              We use our pipeline to create MuLAn-COCO and MuLAn-LAION datasets, which contain a variety of image
              decompositions in terms of style, composition and complexity. With MuLAn, we provide the first
              photorealistic resource providing instance decomposition and occlusion information for high quality images,
              opening up new avenues for text-to-image generative AI research. With this, we aim to encourage the
              development of novel generation and editing technology, in particular layer-wise solutions.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Image carousel -->
<section class="hero is-small">
    <div class="hero-body">
        <div class="container">
        <h2 class="title is-3">MuLAn-COCO</h2>
            <div id="results-carousel_COCO" class="carousel results-carousel">
                <div class="card">
                    <div class="card-image">
                        <img src="static/images/COCO0.png" alt="COCO0" />
                    </div>
                </div>
                <div class="card">
                    <div class="card-image">
                        <img src="static/images/COCO1.png" alt="COCO1" />
                    </div>
                </div>
                <div class="card">
                    <div class="card-image">
                        <img src="static/images/COCO2.png" alt="COCO2" />
                    </div>
                </div>
                <div class="card">
                    <div class="card-image">
                        <img src="static/images/COCO3.png" alt="COCO3" />
                    </div>
                </div>
                <div class="card">
                    <div class="card-image">
                        <img src="static/images/COCO4.png" alt="COCO4" />
                    </div>
                </div>
                <div class="card">
                    <div class="card-image">
                        <img src="static/images/COCO5.png" alt="COCO5" />
                    </div>
                </div>
                 <div class="card">
                    <div class="card-image">
                        <img src="static/images/COCO6.png" alt="COCO6" />
                    </div>
                </div>
                 <div class="card">
                    <div class="card-image">
                        <img src="static/images/COCO7.png" alt="COCO7" />
                    </div>
                </div>
                 <div class="card">
                    <div class="card-image">
                        <img src="static/images/COCO8.png" alt="COCO8" />
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>
<!-- End image carousel -->

<!-- Image carousel -->
<section class="hero is-small">
    <div class="hero-body">
        <div class="container">
        <h2 class="title is-3">MuLAn-LAION</h2>
            <div id="results-carousel_LAION" class="carousel results-carousel">
                <div class="card">
                    <div class="card-image">
                        <img src="static/images/LAION0.png" alt="LAION0" />
                    </div>
                </div>
                <div class="card">
                    <div class="card-image">
                        <img src="static/images/LAION1.png" alt="LAION1" />
                    </div>
                </div>
                <div class="card">
                    <div class="card-image">
                        <img src="static/images/LAION2.png" alt="LAION2" />
                    </div>
                </div>
                <div class="card">
                    <div class="card-image">
                        <img src="static/images/LAION3.png" alt="LAION3" />
                    </div>
                </div>
                <div class="card">
                    <div class="card-image">
                        <img src="static/images/LAION4.png" alt="LAION4" />
                    </div>
                </div>
                <div class="card">
                    <div class="card-image">
                        <img src="static/images/LAION5.png" alt="LAION5" />
                    </div>
                </div>
                 <div class="card">
                    <div class="card-image">
                        <img src="static/images/LAION6.png" alt="LAION6" />
                    </div>
                </div>
                 <div class="card">
                    <div class="card-image">
                        <img src="static/images/LAION7.png" alt="LAION7" />
                    </div>
                </div>
                 <div class="card">
                    <div class="card-image">
                        <img src="static/images/LAION8.png" alt="LAION8" />
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>
<!-- End image carousel -->

<section class="hero is-small">
        <div class="hero-body">
            <div class="container ">
                <h2 class="title is-3">Decomposition Pipeline</h2>
                <p style="margin-bottom:1cm;">To build MuLAn, we design an image processing pipeline that takes as input a
                single RGB image and outputs a multi-layer RGBA decom-
                position of its background and individual object instances.
                We propose to leverage large-scale pre-trained foundational
                models to build a robust, general purpose pipeline without
                incurring any additional model training costs. We separate
                our decomposition process into three submodules, focusing
                on 1) instance identification, ordering and extraction, 2) in-
                stance completion of occluded appearance, and 3) image
                re-assembly as an RGBA stack. Each submodule is carefully
                designed to ensure general applicability, high instance and
                background reconstruction quality, and maximal consistency
                between input image and composed RGBA stack.
                </p>
            <div>
              <img id="method_sliding" src="static/images/method_1.png">
            </div>
            </div>
        </div>
    </section>

<section class="hero is-small">
        <div class="hero-body">
            <div class="container ">
                <h2 class="title is-3">Dataset Description</h2>
                <p style="margin-bottom:1cm;"><b>Dataset size</b>: MuLAn is split into two subsets comprising annotations from the MuLAn and LAION datasets.
                    MuLAn-COCO consists of 16,034 images with 40,335 instances, while MuLAn-LAION
                    consists of 28,826 images with 60,934 instances, for a total of 44,862 images with 101,269 instances.
                </p>
                <p style="margin-bottom:1cm;"> <b>Dataset format </b>: MuLAn decompositions are released as annotations,
                    comprising original image metadata and decomposition results necessary to reconstruct RGBA stacks.

                    Layers are indexed from 0 to N with Layer 0 being the background. All layer annotations comprise
                    the masks required to extract original image content, the inpainted occluded content, and instance
                    transparency Alpha layers for them. Each individual layer has an associated COCO style short caption.
                    For the background (Layer 0), the original image and the recomposed image, we additionally
                    provide LLaVa style long captions. The annotations are organised as follows:

                </p>
            </div>
        </div>

<pre>
    <code>

                "captioning": {

                            "llava": LLaVa model details
                            "blip2": BLIP 2 model details
                            "clip": CLIP model details
                  }
                  "background": {

                            "llava": Detailed background LLaVa caption
                            "blip2": COCO style BLIP 2 background caption chosen by CLIP
                            "original_image_mask": Original image background content mask
                            "inpainted_delta": Additive inpainted background content


                  }
                  "image": {

                            "llava": Detailed original image LLaVa caption
                            "blip2": COCO style BLIP 2 original image  caption chosen by CLIP.


                  }
                  "instances": {

                            "blip2": COCO style BLIP 2 instance caption chosen by CLIP.
                            "original_image_mask": Original image instance content mask
                            "inpainted_delta": Additive inpainted instance content
                            "instance_alpha": Alpha layer of the inpainted instance


                  }
        </code>
</pre>

<section class="hero is-small">
        <div class="hero-body">
            <div class="container ">
                <h2 class="title is-3">Dataset Applications</h2>
                <p style="margin-bottom:1cm;">We evaluated the potential of MuLAn via two applications. First, training
                    an InstructPix2Pix model to add instances to an image. Using MuLAn, we are able to successfully inpute
                    new instances with substantially better robustness to attribute leakage.
                </p>
                <img loading="lazy" src="static/images/mulan_ip2p.webp" alt="ip2p" />
                <p style="margin-bottom:1cm;margin-top:1cm;">Second, we fine-tuned a stable diffusion model to generate RGBA images using
                    the 100K+ instances in MuLAn. In comparison with the original model, and a model fine-tuned on a
                    collection of matting datasets, we are able to generate better instances and achieve a more accurate
                    understanding of transparency.
                </p>
                <img loading="lazy" src="static/images/rgba-generation.webp" alt="rgba" />
            </div>
        </div>
</section>


<!--&lt;!&ndash; Youtube video &ndash;&gt;-->
<!--<section class="hero is-small is-light">-->
<!--  <div class="hero-body">-->
<!--    <div class="container">-->
<!--      &lt;!&ndash; Paper video. &ndash;&gt;-->
<!--      <h2 class="title is-3">Video Presentation</h2>-->
<!--      <div class="columns is-centered has-text-centered">-->
<!--        <div class="column is-four-fifths">-->
<!--          -->
<!--          <div class="publication-video">-->
<!--            &lt;!&ndash; Youtube embed code here &ndash;&gt;-->
<!--            <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>-->
<!--          </div>-->
<!--        </div>-->
<!--      </div>-->
<!--    </div>-->
<!--  </div>-->
<!--</section>-->
<!--&lt;!&ndash; End youtube video &ndash;&gt;-->








<!--&lt;!&ndash; Paper poster &ndash;&gt;-->
<!--<section class="hero is-small is-light">-->
<!--  <div class="hero-body">-->
<!--    <div class="container">-->
<!--      <h2 class="title">Poster</h2>-->

<!--      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">-->
<!--          </iframe>-->
<!--        -->
<!--      </div>-->
<!--    </div>-->
<!--  </section>-->
<!--&lt;!&ndash;End paper poster &ndash;&gt;-->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code@InProceedings{Tudosiu_2024_CVPR,
    author    = {Tudosiu, Petru-Daniel and Yang, Yongxin and Zhang, Shifeng and Chen, Fei and McDonagh, Steven and Lampouras, Gerasimos and Iacobacci, Ignacio and Parisot, Sarah},
    title     = {MULAN: A Multi Layer Annotated Dataset for Controllable Text-to-Image Generation},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2024},
    pages     = {22413-22422}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->
    <script>
      document.addEventListener('DOMContentLoaded', (event) => {
      let images = ['static/images/banner_1.png', 'static/images/banner_2.png', 'static/images/banner_3.png', 'static/images/banner_4.png']; // Paths to your images
      let currentIndex = 0;
  
      setInterval(() => {
          currentIndex++;
          if (currentIndex >= images.length) currentIndex = 0; // Loop back to the first image
          document.getElementById('banner_sliding').src = images[currentIndex];
      }, 1000); // Change image every 1 second
  });

    document.addEventListener('DOMContentLoaded', (event) => {
        let images = ['static/images/method_1.png', 'static/images/method_2.png', 'static/images/method_3.png']; // Paths to your images
        let currentIndex = 0;
    
        setInterval(() => {
            currentIndex++;
            if (currentIndex >= images.length) currentIndex = 0; // Loop back to the first image
            document.getElementById('method_sliding').src = images[currentIndex];
        }, 2000); // Change image every 2 seconds
    });
    </script>
  </body>
  </html>
